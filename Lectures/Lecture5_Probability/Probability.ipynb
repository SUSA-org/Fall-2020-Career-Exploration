{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Probability\n",
    "\n",
    "Probability is an exceptionally important foundational topic in statistics and computer science. Probability tries to answer: _given knowledge about the population, what can I conclude about a random sample?_ Here, we will not be going too deeply into probability (that will be the role of Stat 134, Stat 140, or CS 70). Instead, we will be giving you a taste of probability for those who are planning to dive into these topics in the future. If you aren't already stoked, here are just a few of the countless applications of probability:\n",
    "\n",
    "+ Machine learning\n",
    "+ Finance\n",
    "+ Robotics\n",
    "+ Hardware engineering\n",
    "+ Gambling strategies\n",
    "+ Polling predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this notebook, we use capital letters for random events, or random variables. Therefore, we define $\\mathbb{P}(X=x)$ as the probability of the random event $X$ is actually $x$. To make things more concrete, let $D$ be the number that shows up after we roll a die. Then $\\mathbb{P}(D=5)$ is the probability that we roll a $5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could get more creative with this notation:\n",
    "\n",
    "+ $\\mathbb{P}(X>x)$ is the probability that event $X$ is greater than $x$.\n",
    "+ $\\mathbb{P}(X\\neq x)$ is the probability that event $X$ is not $x$.\n",
    "+ $\\mathbb{P}(X = x, Y \\geq y)$ is the probability that event $X$ is $x$ AND event $Y$ is greater than or equal to $y$.\n",
    "+ $\\mathbb{P}(X = cloudy)$ is the probability that it is cloudy. Yep, it doesn't even have to be a math expression, just needs to be an event!\n",
    "+ $\\mathbb{P}(X = x | Y = y)$ is the probability that event $X$ is $x$, GIVEN that event $Y$ is $y$. Known as a conditional probability, this is arguably the be most confusing to understand, so we dedicate the next section to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "Let's start with a quick example. Say I have a fair coin and a biased coin that shows heads 75% of the time. Since they look identical, you choose one with equal probability and flip it. If it's heads, you get 10 dollars, and if it's tails, you get nothing. If I want to know your chance of winning 10 dollars, I would need to calculate $\\mathbb{P}(W=10)$ where $W$ is your winnings. Here, I have no clue what coin you chose, so we need to calculate everything (chance you pick the fair coin and win plus the chance you pick the biased coin and win). Now suppose I know for a fact you chose the fair coin and I want to calculate your chance of winning 10 dollars. Then, letting $C$ be the coin you chose, I would need to find $\\mathbb{P}(W=10|C=fair)$ which is just $\\frac{1}{2}$. Do you see the difference between $\\mathbb{P}(W=10)$ and $\\mathbb{P}(W=10|C=fair)$?\n",
    "\n",
    "In summary, conditional probability is the probability of something happening given some additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication Rule\n",
    "\n",
    "To calculate the probability of 2 events happening, we can decompose $\\mathbb{P}(A, B)$:\n",
    "\n",
    "$\\mathbb{P}(A, B) = \\mathbb{P}(A| B) \\mathbb{P}(B) = \\mathbb{P}(B|A)\\mathbb{P}(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence\n",
    "\n",
    "Mathematically, we say is two events $A$ and $B$ are independent if and only if:\n",
    "\n",
    "$\\mathbb{P}(A, B) = \\mathbb{P}(A) \\mathbb{P}(B)$\n",
    "\n",
    "Intuitively, this means knowing $A$ or $B$ will not tell you anything about the other. If I flip a coin two times, the first flip and second flip are independent of each other. This also means:\n",
    "\n",
    "$\\mathbb{P}(A | B) = \\mathbb{P}(A)$ and $\\mathbb{P}(B | A) = \\mathbb{P}(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "This is one of the most crucial equations statistics, so commit it to memory! It's also derivable from the multiplication rule ;)\n",
    "\n",
    "$\\mathbb{P}(A| B) =  \\frac{\\mathbb{P}(A, B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B|A) \\mathbb{P}(A)}{\\mathbb{P}(B)}$\n",
    "\n",
    "Notice how it essentially flips $A|B$ on the left to $B|A$ on the right. This allows us to flexibility to calculate whichever one is easier and plug it into the appropriate Bayes' equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Check\n",
    "\n",
    "1) I have 2 dice. I roll one and observe that I got $N$. Then, I roll the second one $N$ times, and let $M$ be the maximum of these $N$ rolls. Are $M$ and $N$ independent?\n",
    "\n",
    "2) Assume the chance of giving birth to a boy or girl are each $\\frac{1}{2}$. Given a family has 3 girls already, what is the probability the next one will be a girl? \n",
    "\n",
    "3) I have 3 cards. The first is blue on both sides. The second is gold on both sides. The third is blue on one side and gold on the other. I pick one at random and show you one side of that chosen card. Given that you see that side is gold, what is the probability that the other side is blue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation\n",
    "\n",
    "Expectation of $X$, or $\\mathbb{E}(X)$, is the average outcome of a random event. It is defined as:\n",
    "\n",
    "$\\mathbb{E}(X) = \\sum_{\\text{all }x} x \\mathbb{P}(X=x)$\n",
    "\n",
    "It is important to note that the expected value does not have to be a possible outcome. In fact, in many cases, it is not! For instance, the average roll of a fair die is 3.5.\n",
    "\n",
    "Let $a$ and $b$ be constants and $X$ and $Y$ be random variables, then the following are always true:\n",
    "\n",
    "+ $\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)$ \n",
    "+ $\\mathbb{E}(aX + b) = a \\mathbb{E}(X) + b$\n",
    "\n",
    "Sometimes, you may encounter $\\mathbb{E}(X^n)$. This is called the _$n$th moment of $X$_.\n",
    "\n",
    "$\\mathbb{E}(X^n) = \\sum_{\\text{all }x} x^n \\mathbb{P}(X=x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "Variance of $X$, or $var(X)$, roughly describes the amount of variation in $X$. Mathematically, \n",
    "\n",
    "$var(X) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2$\n",
    "\n",
    "Pretty neat!\n",
    "\n",
    "Let $a$ and $b$ be constants and $X$ be a random variable, then the following are always true:\n",
    "\n",
    "+ $var(X) \\geq 0$\n",
    "+ $var(aX + b) = a^2 var(X)$\n",
    "+ $var(X + Y) = var(X) + var(Y)$  if $X$ and $Y$ are independent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Probability Distributions\n",
    "\n",
    "You can think of probability distributions as a function that takes in an event and returns the probability or probability density of that event happening. We'll talk about probability density later when we get to continuous distributions. Let $f$ be a function, the two criteria for a function to be a discrete probability distribution are:\n",
    "\n",
    "+ $f(x) \\geq 0 \\quad \\forall x$\n",
    "+ $\\sum_{\\text{all } x} f(x) = 1$\n",
    "\n",
    "In the discrete case, we think of $f(x) = \\mathbb{P}(X=x)$. For example, for a fair die, if $X$ is number we roll, $f(x) = \\frac{1}{6}$ if $x \\in \\{1, 2, 3, 4, 5, 6\\}$ and 0 otherwise. We'll explore some well known classes of discrete distributions.\n",
    "\n",
    "#### Uniform\n",
    "\n",
    "A discete uniform distribution is one where each possible outcome is equally likely.\n",
    "\n",
    "$\\mathbb{P}(X = x) = \\frac{1}{n} \\quad \\forall \\text{ possible } x $ where $n$ is the number of possible outcomes.\n",
    "\n",
    "Example: The distribution of a single die roll.\n",
    "\n",
    "#### Binomial\n",
    "\n",
    "A binomial distribution is one where we observe a sequence of $n$ identical random events, and we want to know the probability of a specific outcome happening $k$ times out of $n$. Let $p$ be the probability of this outcome happening for each event.\n",
    "\n",
    "$\\mathbb{P}(X = k) = {n\\choose k} p^k (1-p)^{n-k}$\n",
    "\n",
    "Example: The probability of observing three 5's in 10 dice rolls: ${10\\choose 3} {(\\frac{1}{6})}^3 {(\\frac{5}{6})}^{7}$\n",
    "\n",
    "#### Geometric \n",
    "\n",
    "A geometric distribution is one where we care about the first instance of a sequence of identical random events, and we want to know the probability of an outcome occuring for the first time on the $k$th iteration. Let $p$ be the probability of this outcome happening for each event.\n",
    "\n",
    "$\\mathbb{P}(X = k) = p(1-p)^{k-1}$\n",
    "\n",
    "Example: The probability of seeing the first 5 on a die on the 7th roll: $ (\\frac{1}{6})(\\frac{5}{6})^6$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Probability Distributions\n",
    "\n",
    "Buckle your seatbelts because things get weird in the continuous realm. Generally in the continuous case, $f(x) \\neq \\mathbb{P}(X=x)$. In fact, $\\mathbb{P}(X=x) = 0$ (think: what is the probability of picking an infinitesimally small point on line? answer: 0). Instead, we calculate probabilities of ranges of $X$ such as $\\mathbb{P}(x_1 \\leq X \\leq x_2)$ by integrating $\\int_{x_1}^{x_2} f(x)$. \n",
    "\n",
    "Therefore, the two criteria for a function to be a discrete probability distribution are:\n",
    "\n",
    "+ $f(x) \\geq 0 \\quad \\forall x$\n",
    "+ $\\int_{x} f(x) = 1$\n",
    "\n",
    "For instance, something like $f(x) = 2x \\text{ for } 0 \\leq x \\leq 1$ is a valid continuous distribution. Although the usefulness of such a distribution is handwavy, we'll go into some simple and common ones.\n",
    "\n",
    "#### Uniform\n",
    "\n",
    "A continuous uniform distribution is one where each possible outcome is equally likely.\n",
    "\n",
    "$f(x) = \\frac{1}{b-a} \\text{ for } a \\leq x \\leq b $\n",
    "\n",
    "Example: x-coordinate of a raindrop hitting a square meter tile.\n",
    "\n",
    "#### Normal \n",
    "\n",
    "A normal distribution is your standard bell curve. It is defined by the mean $\\mu$ and standard deviation $\\sigma$:\n",
    "\n",
    "$f(x) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }} e^{ - \\frac{( {x - \\mu } )^2 }{2\\sigma^2}} $\n",
    "\n",
    "Example: Distribution of height\n",
    "\n",
    "Typically, if $X$ is normally distributed with mean $\\mu$ and standard deviation $\\sigma$, we write $X \\sim \\mathcal{N}(\\mu, \\sigma ^2)$. \n",
    "\n",
    "If $X \\sim \\mathcal{N}(\\mu_x, \\sigma^2_x)$, $Y \\sim \\mathcal{N}(\\mu_y, \\sigma ^2_y)$, and $X$ and $Y$ are independent, then $X+Y \\sim \\mathcal{N}(\\mu_x + \\mu_y, \\sigma^2_x + \\sigma^2_y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Distributions\n",
    "\n",
    "Joint distributions are distributions involving multiple random variables. These random variables can be independent or dependent. In fact, we've already touched on joint distributions throughout this lecture. Recall the example with the fair and biased coins in the Conditional Probability section. $W$ and $C$ together is a joint distribution. Another example of a joint distribution that you will use soon is the multivariate normal (multiple normal distributions that may be dependent on each other), the bedrock of linear regression. \n",
    "\n",
    "The math can easily get messy when working with joint distributions, so we'll skip it for your future classes to cover ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final note\n",
    "\n",
    "Probability can get very tricky very quickly. While the rest of the lectures will not go deeply into probability and will only use select portions of today's lecture, further scrutinization will reveal the inseparability of probability with much of statistics and computer science. For example, soon, you will be learning about linear regression. Indeed, it is possible to learn about linear regression without probability (it is just a formula after all!), but as the good statisticians we all are, it is imperative to know our models' assumptions and inner workings. While it may not be immediately obvious, probability demystifies these topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
